{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#推荐系统\" data-toc-modified-id=\"推荐系统-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>推荐系统</a></span><ul class=\"toc-item\"><li><span><a href=\"#推荐系统的评价\" data-toc-modified-id=\"推荐系统的评价-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>推荐系统的评价</a></span></li><li><span><a href=\"#基于内容的推荐\" data-toc-modified-id=\"基于内容的推荐-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>基于内容的推荐</a></span><ul class=\"toc-item\"><li><span><a href=\"#特征提取\" data-toc-modified-id=\"特征提取-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>特征提取</a></span><ul class=\"toc-item\"><li><span><a href=\"#结构化数据\" data-toc-modified-id=\"结构化数据-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>结构化数据</a></span></li><li><span><a href=\"#非结构化数据\" data-toc-modified-id=\"非结构化数据-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>非结构化数据</a></span></li></ul></li><li><span><a href=\"#用户偏好计算\" data-toc-modified-id=\"用户偏好计算-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>用户偏好计算</a></span></li><li><span><a href=\"#内容召回\" data-toc-modified-id=\"内容召回-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>内容召回</a></span></li><li><span><a href=\"#物品排序\" data-toc-modified-id=\"物品排序-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>物品排序</a></span></li></ul></li><li><span><a href=\"#基于领域的推荐（协同算法）\" data-toc-modified-id=\"基于领域的推荐（协同算法）-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>基于领域的推荐（协同算法）</a></span><ul class=\"toc-item\"><li><span><a href=\"#UserCF和ItemCF的区别\" data-toc-modified-id=\"UserCF和ItemCF的区别-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>UserCF 和 ItemCF 的区别</a></span><ul class=\"toc-item\"><li><span><a href=\"#推荐场景\" data-toc-modified-id=\"推荐场景-1.3.1.1\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>推荐场景</a></span></li><li><span><a href=\"#系统多样性\" data-toc-modified-id=\"系统多样性-1.3.1.2\"><span class=\"toc-item-num\">1.3.1.2&nbsp;&nbsp;</span>系统多样性</a></span></li><li><span><a href=\"#用户特点对推荐算法影响的比较\" data-toc-modified-id=\"用户特点对推荐算法影响的比较-1.3.1.3\"><span class=\"toc-item-num\">1.3.1.3&nbsp;&nbsp;</span>用户特点对推荐算法影响的比较</a></span></li></ul></li><li><span><a href=\"#基于用户的协同过滤算法\" data-toc-modified-id=\"基于用户的协同过滤算法-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>基于用户的协同过滤算法</a></span></li><li><span><a href=\"#基于物品的协同过滤算法\" data-toc-modified-id=\"基于物品的协同过滤算法-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>基于物品的协同过滤算法</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于共同喜欢物品的用户列表计算（购买数）:\" data-toc-modified-id=\"基于共同喜欢物品的用户列表计算（购买数）:-1.3.3.1\"><span class=\"toc-item-num\">1.3.3.1&nbsp;&nbsp;</span>基于共同喜欢物品的用户列表计算（购买数）:</a></span></li><li><span><a href=\"#基于余弦的相似度（评分数据）\" data-toc-modified-id=\"基于余弦的相似度（评分数据）-1.3.3.2\"><span class=\"toc-item-num\">1.3.3.2&nbsp;&nbsp;</span>基于余弦的相似度（评分数据）</a></span></li><li><span><a href=\"#热门物品的惩罚\" data-toc-modified-id=\"热门物品的惩罚-1.3.3.3\"><span class=\"toc-item-num\">1.3.3.3&nbsp;&nbsp;</span>热门物品的惩罚</a></span></li><li><span><a href=\"#计算用户对商品的推荐分数\" data-toc-modified-id=\"计算用户对商品的推荐分数-1.3.3.4\"><span class=\"toc-item-num\">1.3.3.4&nbsp;&nbsp;</span>计算用户对商品的推荐分数</a></span></li></ul></li><li><span><a href=\"#基于模型的协同过滤\" data-toc-modified-id=\"基于模型的协同过滤-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>基于模型的协同过滤</a></span><ul class=\"toc-item\"><li><span><a href=\"#关联算法\" data-toc-modified-id=\"关联算法-1.3.4.1\"><span class=\"toc-item-num\">1.3.4.1&nbsp;&nbsp;</span>关联算法</a></span></li><li><span><a href=\"#聚类算法\" data-toc-modified-id=\"聚类算法-1.3.4.2\"><span class=\"toc-item-num\">1.3.4.2&nbsp;&nbsp;</span>聚类算法</a></span></li></ul></li><li><span><a href=\"#基于矩阵分解的推荐方法\" data-toc-modified-id=\"基于矩阵分解的推荐方法-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>基于矩阵分解的推荐方法</a></span><ul class=\"toc-item\"><li><span><a href=\"#奇异值分解\" data-toc-modified-id=\"奇异值分解-1.3.5.1\"><span class=\"toc-item-num\">1.3.5.1&nbsp;&nbsp;</span>奇异值分解</a></span></li><li><span><a href=\"#SVD梳理流程如下：\" data-toc-modified-id=\"SVD梳理流程如下：-1.3.5.2\"><span class=\"toc-item-num\">1.3.5.2&nbsp;&nbsp;</span>SVD 梳理流程如下：</a></span></li><li><span><a href=\"#隐语义模型\" data-toc-modified-id=\"隐语义模型-1.3.5.3\"><span class=\"toc-item-num\">1.3.5.3&nbsp;&nbsp;</span>隐语义模型</a></span></li></ul></li><li><span><a href=\"#基于稀疏自编码的推荐方法\" data-toc-modified-id=\"基于稀疏自编码的推荐方法-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>基于稀疏自编码的推荐方法</a></span><ul class=\"toc-item\"><li><span><a href=\"#基础自编码结构\" data-toc-modified-id=\"基础自编码结构-1.3.6.1\"><span class=\"toc-item-num\">1.3.6.1&nbsp;&nbsp;</span>基础自编码结构</a></span></li><li><span><a href=\"#多层结构\" data-toc-modified-id=\"多层结构-1.3.6.2\"><span class=\"toc-item-num\">1.3.6.2&nbsp;&nbsp;</span>多层结构 </a></span></li></ul></li></ul></li><li><span><a href=\"#基于社交网络的推荐算法\" data-toc-modified-id=\"基于社交网络的推荐算法-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>基于社交网络的推荐算法</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于用户的推荐在社交网络中的应用\" data-toc-modified-id=\"基于用户的推荐在社交网络中的应用-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>基于用户的推荐在社交网络中的应用</a></span><ul class=\"toc-item\"><li><span><a href=\"#相似度计算方法\" data-toc-modified-id=\"相似度计算方法-1.4.1.1\"><span class=\"toc-item-num\">1.4.1.1&nbsp;&nbsp;</span>相似度计算方法 </a></span></li><li><span><a href=\"#新的相似度分数\" data-toc-modified-id=\"新的相似度分数-1.4.1.2\"><span class=\"toc-item-num\">1.4.1.2&nbsp;&nbsp;</span>新的相似度分数 </a></span></li></ul></li><li><span><a href=\"#node2vec技术在社交网络推荐中的应用\" data-toc-modified-id=\"node2vec技术在社交网络推荐中的应用-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>node2vec 技术在社交网络推荐中的应用</a></span><ul class=\"toc-item\"><li><span><a href=\"#random-walk\" data-toc-modified-id=\"random-walk-1.4.2.1\"><span class=\"toc-item-num\">1.4.2.1&nbsp;&nbsp;</span>random walk</a></span></li><li><span><a href=\"#Work2vec\" data-toc-modified-id=\"Work2vec-1.4.2.2\"><span class=\"toc-item-num\">1.4.2.2&nbsp;&nbsp;</span>Work2vec</a></span></li></ul></li></ul></li><li><span><a href=\"#推荐系统冷启动问题\" data-toc-modified-id=\"推荐系统冷启动问题-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>推荐系统冷启动问题</a></span><ul class=\"toc-item\"><li><span><a href=\"#用户冷启动\" data-toc-modified-id=\"用户冷启动-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>用户冷启动</a></span><ul class=\"toc-item\"><li><span><a href=\"#使用账户信息\" data-toc-modified-id=\"使用账户信息-1.5.1.1\"><span class=\"toc-item-num\">1.5.1.1&nbsp;&nbsp;</span>使用账户信息</a></span></li><li><span><a href=\"#使用手机IMEI号\" data-toc-modified-id=\"使用手机IMEI号-1.5.1.2\"><span class=\"toc-item-num\">1.5.1.2&nbsp;&nbsp;</span>使用手机 IMEI 号</a></span></li><li><span><a href=\"#制造选项\" data-toc-modified-id=\"制造选项-1.5.1.3\"><span class=\"toc-item-num\">1.5.1.3&nbsp;&nbsp;</span>制造选项</a></span></li></ul></li><li><span><a href=\"#物品冷启动\" data-toc-modified-id=\"物品冷启动-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>物品冷启动</a></span><ul class=\"toc-item\"><li><span><a href=\"#利用物品的内容信息\" data-toc-modified-id=\"利用物品的内容信息-1.5.2.1\"><span class=\"toc-item-num\">1.5.2.1&nbsp;&nbsp;</span>利用物品的内容信息</a></span></li><li><span><a href=\"#利用专家的标注数据\" data-toc-modified-id=\"利用专家的标注数据-1.5.2.2\"><span class=\"toc-item-num\">1.5.2.2&nbsp;&nbsp;</span>利用专家的标注数据</a></span></li></ul></li><li><span><a href=\"#深度学习技术在物品冷启动上的应用\" data-toc-modified-id=\"深度学习技术在物品冷启动上的应用-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>深度学习技术在物品冷启动上的应用 </a></span><ul class=\"toc-item\"><li><span><a href=\"#案例一：CNN在音频流派分类上的应用\" data-toc-modified-id=\"案例一：CNN在音频流派分类上的应用-1.5.3.1\"><span class=\"toc-item-num\">1.5.3.1&nbsp;&nbsp;</span>案例一：CNN 在音频流派分类上的应用 </a></span></li><li><span><a href=\"#案例二：人脸魅力值打分在视频推荐中的应用\" data-toc-modified-id=\"案例二：人脸魅力值打分在视频推荐中的应用-1.5.3.2\"><span class=\"toc-item-num\">1.5.3.2&nbsp;&nbsp;</span>案例二：人脸魅力值打分在视频推荐中的应用</a></span></li></ul></li></ul></li><li><span><a href=\"#CTR预估算法\" data-toc-modified-id=\"CTR预估算法-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>CTR 预估算法</a></span><ul class=\"toc-item\"><li><span><a href=\"#FM（因子分解机）算法\" data-toc-modified-id=\"FM（因子分解机）算法-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>FM（因子分解机）算法</a></span></li><li><span><a href=\"#FFM（场感知分解机）算法\" data-toc-modified-id=\"FFM（场感知分解机）算法-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>FFM（场感知分解机）算法</a></span></li><li><span><a href=\"#FNN算法\" data-toc-modified-id=\"FNN算法-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>FNN 算法</a></span></li><li><span><a href=\"#DeepFM算法\" data-toc-modified-id=\"DeepFM算法-1.6.4\"><span class=\"toc-item-num\">1.6.4&nbsp;&nbsp;</span>DeepFM 算法</a></span></li></ul></li><li><span><a href=\"#基于深度学习的推荐模型\" data-toc-modified-id=\"基于深度学习的推荐模型-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>基于深度学习的推荐模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于DeepFM的推荐算法\" data-toc-modified-id=\"基于DeepFM的推荐算法-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>基于 DeepFM 的推荐算法</a></span></li></ul></li><li><span><a href=\"#推荐系统架构设计\" data-toc-modified-id=\"推荐系统架构设计-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>推荐系统架构设计</a></span><ul class=\"toc-item\"><li><span><a href=\"#推荐系统的基本模型\" data-toc-modified-id=\"推荐系统的基本模型-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>推荐系统的基本模型 </a></span></li><li><span><a href=\"#推荐系统常见架构\" data-toc-modified-id=\"推荐系统常见架构-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>推荐系统常见架构 </a></span><ul class=\"toc-item\"><li><span><a href=\"#基于离线训练的推荐系统架构设计\" data-toc-modified-id=\"基于离线训练的推荐系统架构设计-1.8.2.1\"><span class=\"toc-item-num\">1.8.2.1&nbsp;&nbsp;</span>基于离线训练的推荐系统架构设计 </a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__推荐系统需同时具备：__\n",
    ">速度快  \n",
    ">准确度高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color : red'>__热门程度和实效性是个性化新闻的推荐重点，而个性化是补充__</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推荐系统的评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的推测不代表是好的推荐！如果本来用户就有了主观选择趋势，此时的预测实际上并不能提高潜在商品的被选择率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 评测方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1、离线实验  \n",
    ">缺点：较难得到离线的与商业指标相关的指标  \n",
    "\n",
    ">2、用户调查  \n",
    ">缺点：用户量大时，成本过高；用户量小时，得出的结论往往没有统计意义\n",
    "\n",
    ">3、在线实验（较常使用A/B test）  \n",
    ">缺点：周期较长，必须进行长期的实验才有可靠的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于内容的推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__优点：__\n",
    ">1、易于定位问题  \n",
    ">2、能为具有特殊兴趣爱好的用户进行推荐  \n",
    ">3、物品没有冷启动问题    \n",
    "\n",
    "__缺点：__\n",
    ">1、要求取出的特征必须具有良好的结构性  \n",
    ">2、推荐精度低，相同内容特征的物品差异性不大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取\n",
    "#### 结构化数据\n",
    ">可取用二进制的方法进行表示\n",
    "#### 非结构化数据\n",
    ">1、对于英文，可直接取词；对于中文，需先直接分词  \n",
    ">2、统计方法分两种  \n",
    ">>2.1、基础统计法：商品或词汇出现就取1，没出现就取0（当很多文章都包含某个词时，则这个词没有信息量）   \n",
    ">><span style='color:red'>__2.2、词频统计法：__</span>  \n",
    "$$\n",
    "w_{k,j}=\\frac{TF-IDF(t_{k},d_{j})}{\\sqrt{\\sum{TF-IDF(t_{k},d_{j})^{2}}}}\n",
    "$$  \n",
    "$w_{k,j}$是 词k在文章j中的权重  \n",
    "$$TF-IDF(t_{k},d_{j})=TF(t_{k},d_{j})\\cdot\\log\\frac{N}{n_{k}}$$\n",
    "$TF(t_{k},d_{j})$是词k在商品或文章j中出现的次数   \n",
    "$n_{k}$是包含词k的文章的数量  \n",
    "<span style='color:blue'>__由上得出每篇文章的内容特征向量：__</span> $$d_{j}=(w_{1j},w_{2j},…,w_{nj})$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户偏好计算\n",
    "<span style='color:blue'>__计算客户的文章偏好时，可直接取用户x喜欢文章的向量平均值：__</span>\n",
    "$$U_{x}=\\frac{(d_{x1}+d_{x2}+d_{x3})}{3}=(u_{1x},u_{2x},…,u_{nx})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内容召回\n",
    "<span style='color:blue'>__则用户x在文章t上的得分为：__</span>\n",
    "$$cos\\theta=\\frac{U_{x}\\cdot\\ d_{t}}{\\lVert U_{k} \\rVert\\lVert d_{t} \\rVert}=\\frac{\\sum\\limits_{i=1}^n (u_{ix}\\cdot\\ w_{it})}{\\sqrt{\\sum\\limits_{i=1}^n u_{ix}^2}\\cdot\\ \\sqrt{\\sum\\limits_{i=1}^n w_{it}^2}}$$\n",
    ">代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T18:12:06.484006Z",
     "start_time": "2019-06-06T18:12:05.724504Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cossim(u_x,d_t):\n",
    "    num1=np.dot(u_x,d_t)\n",
    "    num2=np.linalg(u_x)*np.linalg(d_t)\n",
    "    return num1/num2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 物品排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于领域的推荐（协同算法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UserCF和ItemCF的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推荐场景\n",
    ">当用户数量远远超过商品的数量，选用ItemCF，如购物网站、图书、电子商务、电影网站等\n",
    ">>该场景情况下，往往用户的兴趣爱好比较固定，用户数量往往也比商品数量多，此时若使用UserCF，空间复杂度消耗更大\n",
    ">>>给用户推荐时，因为与使用过类似的商品，此时的解释更具有说服力。但如果说某人也购买过此类产品，此时用户并不认识此人，难以有信服力\n",
    "\n",
    ">当商品或内容的更新频率非常高时，选用UserCF，如社交网络、新闻等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 系统多样性\n",
    "- 也叫 覆盖率，指一个推荐系统能否给用户提供多种选择\n",
    ">ItemCF的多样性UserCF好，因为UserCF更倾向于推荐热门的物品，即ItemCF更容易发现并推荐长尾里的物品  \n",
    "><span style='color : red'>__ItemCF相对单个用户而言，显然多样性不足，但对整个系统而言，因为不同用户的主要兴趣点不同，因此系统的覆盖率会比较好__</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用户特点对推荐算法影响的比较\n",
    ">对于UserCF，如果用户暂时找不到兴趣相投的邻居，UserCF的推荐效果会大打折扣，即UserCF算法跟有多少邻居是成正比关系的  \n",
    ">对于ItemCF，前提是用户喜欢他以前购买过的物品相同类型的物品。<span style='color:red'>针对这个问题可以计算一个用户喜欢的物品的自相似度</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于用户的协同过滤算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T18:12:10.549698Z",
     "start_time": "2019-06-06T18:12:10.469646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'collections.defaultdict'>, {'A': defaultdict(None, {'F': 0.8164965809277261, 'D': 0.4082482904638631, 'B': 0.8164965809277261, 'C': 0.6666666666666666}), 'F': defaultdict(None, {'A': 0.8164965809277261, 'D': 0.5, 'B': 0.5, 'C': 0.4082482904638631}), 'B': defaultdict(None, {'A': 0.8164965809277261, 'F': 0.5, 'C': 0.4082482904638631}), 'D': defaultdict(None, {'A': 0.4082482904638631, 'C': 0.4082482904638631, 'E': 0.5, 'F': 0.5}), 'E': defaultdict(None, {'D': 0.5, 'C': 0.4082482904638631}), 'C': defaultdict(None, {'A': 0.6666666666666666, 'D': 0.4082482904638631, 'E': 0.4082482904638631, 'B': 0.4082482904638631, 'F': 0.4082482904638631})})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def item_index(train):\n",
    "    itemdict=defaultdict(defaultdict)\n",
    "    for key in train:\n",
    "        for i,j in train[key].items():\n",
    "            itemdict[i][key]=j\n",
    "    return itemdict\n",
    "\n",
    "def usersim(itemdict):\n",
    "    n=dict()\n",
    "    c=defaultdict(defaultdict)\n",
    "    w=defaultdict(defaultdict)\n",
    "    for key in itemdict:\n",
    "        for i,j in itemdict[key].items():\n",
    "            if i not in n.keys():\n",
    "                n[i]=0\n",
    "            n[i]+=1\n",
    "            for m in itemdict[key].keys():\n",
    "                if i==m:\n",
    "                    continue\n",
    "                if m not in c[i].keys():\n",
    "                    c[i][m]=0\n",
    "                c[i][m]+=1\n",
    "    for i,items in c.items():\n",
    "        for j,nums in items.items():\n",
    "            w[i][j]=nums/math.sqrt(n[i]*n[j])\n",
    "    return w\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train_data={\n",
    "        'A':{'i1':1,'i2':1 ,'i4':1},\n",
    "        'B':{'i1':1,'i4':1},\n",
    "        'C':{'i1':1,'i2':1,'i5':1},\n",
    "        'D':{'i2':1,'i3':1},\n",
    "        'E':{'i3':1,'i5':1},\n",
    "        'F':{'i2':1,'i4':1}\n",
    "        }\n",
    "    w=usersim(item_index(train_data))\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于物品的协同过滤算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***核心思想：给用户推荐那些和他们之前喜欢的物品相似的物品***  \n",
    "***主要利用了用户行为的集体智慧***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于共同喜欢物品的用户列表计算（购买数）:\n",
    "$$w_{ij}=\\frac{\\left|N(i)\\bigcap N(j)\\right|}{\\sqrt{\\left|N(i)\\right|*\\left|N(j)\\right|}}$$\n",
    "$N(i)$是购买物品i的用户数  \n",
    "$N(j)$是购买物品j的用户数  \n",
    ">代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T18:12:12.307862Z",
     "start_time": "2019-06-06T18:12:12.236815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i4': {'i2': 0.5773502691896258, 'i1': 0.6666666666666667}, 'i1': {'i4': 0.6666666666666667, 'i5': 0.40824829046386296, 'i2': 0.5773502691896258}, 'i2': {'i4': 0.5773502691896258, 'i1': 0.5773502691896258, 'i5': 0.35355339059327373, 'i3': 0.35355339059327373}, 'i5': {'i3': 0.4999999999999999, 'i2': 0.35355339059327373, 'i1': 0.40824829046386296}, 'i3': {'i5': 0.4999999999999999, 'i2': 0.35355339059327373}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def itemsim(train):\n",
    "    c=dict()##物品对的购买数\n",
    "    n=dict()##各个物品的购买数\n",
    "    for _,items in train.items():\n",
    "        for i in items.keys():\n",
    "            if i not in n.keys():\n",
    "                n[i]=0\n",
    "            n[i]+=1\n",
    "            if i not in c.keys():\n",
    "                c[i]=dict()\n",
    "            for j in items.keys():\n",
    "                if i==j:\n",
    "                    continue\n",
    "                if j not in c[i].keys():\n",
    "                    c[i][j]=0\n",
    "                c[i][j]+=1\n",
    "    w=dict()\n",
    "    for i,items in c.items():\n",
    "        if i not in w.keys():\n",
    "            w[i]=dict()\n",
    "        for j,cji in items.items():\n",
    "            w[i][j]=cji/(math.pow(n[i],0.5)*math.pow(n[j],0.5))\n",
    "    return w\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train_data={\n",
    "        'A':{'i1':1,'i2':1 ,'i4':1},\n",
    "        'B':{'i1':1,'i4':1},\n",
    "        'C':{'i1':1,'i2':1,'i5':1},\n",
    "        'D':{'i2':1,'i3':1},\n",
    "        'E':{'i3':1,'i5':1},\n",
    "        'F':{'i2':1,'i4':1}\n",
    "        }\n",
    "    w=itemsim(train_data)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于余弦的相似度（评分数据）\n",
    "<span style='color:red'>__当用户购买了却不喜欢该商品时，基于购买物品的用户列表会出现推荐错误__</span>  \n",
    "$$w_{ij}=\\frac{\\sum\\limits^{len}_{k=1}(n_{ki}\\cdot n_{kj})}{\\sqrt{\\sum\\limits^{len}_{k=1}n_{ki}^2}\\cdot   \\sqrt{\\sum\\limits^{len}_{k=1}n_{kj}^2}}$$  \n",
    "$n_{ki}$是用户k对物品i的评分\n",
    ">代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T18:12:12.951287Z",
     "start_time": "2019-06-06T18:12:12.882242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i4': {'i2': 0.5773502691896258, 'i1': 0.6666666666666667}, 'i1': {'i4': 0.6666666666666667, 'i5': 0.40824829046386296, 'i2': 0.5773502691896258}, 'i2': {'i4': 0.5773502691896258, 'i1': 0.5773502691896258, 'i5': 0.35355339059327373, 'i3': 0.35355339059327373}, 'i5': {'i3': 0.4999999999999999, 'i2': 0.35355339059327373, 'i1': 0.40824829046386296}, 'i3': {'i5': 0.4999999999999999, 'i2': 0.35355339059327373}}\n"
     ]
    }
   ],
   "source": [
    "def itemcos(train):\n",
    "    c=dict()\n",
    "    n=dict()\n",
    "    for _,items in train.items():\n",
    "        for i in items.keys():\n",
    "            if i not in c.keys():\n",
    "                c[i]=dict()\n",
    "            if i not in n.keys():\n",
    "                n[i]=0\n",
    "            n[i]+=items[i]*items[i]\n",
    "            for j in items.keys():\n",
    "                if i==j:\n",
    "                    continue\n",
    "                if j not in c[i].keys():\n",
    "                    c[i][j]=0\n",
    "                c[i][j]+=items[i]*items[j]\n",
    "    w=dict()\n",
    "    for i,items in c.items():\n",
    "        if i not in w.keys():\n",
    "            w[i]=dict()\n",
    "        for j in items.keys():\n",
    "            w[i][j]=c[i][j]/(math.sqrt(n[i])*math.sqrt(n[j]))\n",
    "    return w\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    train = {'A':{'i1':1,'i2':1 ,'i4':1},  \n",
    "     'B':{'i1':1,'i4':1},  \n",
    "     'C':{'i1':1,'i2':1,'i5':1},\n",
    "     'D':{'i2':1,'i3':1},\n",
    "     'E':{'i3':1,'i5':1},\n",
    "     'F':{'i2':1,'i4':1}\n",
    "        }  \n",
    "    w=itemcos(train)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 热门物品的惩罚\n",
    ">PS:分母中使用了物品总购买人数做惩罚，因为热门商品经常与其他商品进行一起购买，除以总人数可以降低该商品与其他商品的相似分数\n",
    "$$w_{ij}=\\frac{\\left|N(i)\\bigcap N(j)\\right|}{\\left|N(i)\\right|^\\alpha \\cdot\\left|N(j)\\right|^{1-\\alpha}}$$\n",
    "$\\alpha$是物品i的惩罚系数，取（0.5,1）\n",
    ">代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T18:12:13.600716Z",
     "start_time": "2019-06-06T18:12:13.525669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i4': {'i2': 0.3512393165574119, 'i1': 0.4295960099848361}, 'i1': {'i4': 0.4295960099848361, 'i5': 0.2852949765682842, 'i2': 0.3512393165574119}, 'i2': {'i4': 0.3512393165574119, 'i1': 0.3512393165574119, 'i5': 0.23325824788420185, 'i3': 0.23325824788420185}, 'i5': {'i3': 0.37892914162759955, 'i2': 0.23325824788420185, 'i1': 0.2852949765682842}, 'i3': {'i5': 0.37892914162759955, 'i2': 0.23325824788420185}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def itemcos_alpha(train,alpha=0.7):\n",
    "    c=dict()\n",
    "    n=dict()\n",
    "    for _,items in train.items():\n",
    "        for i in items.keys():\n",
    "            if i not in c.keys():\n",
    "                c[i]=dict()\n",
    "            if i not in n.keys():\n",
    "                n[i]=0\n",
    "            n[i]+=1\n",
    "            for j in items.keys():\n",
    "                if i==j:\n",
    "                    continue\n",
    "                if j not in c[i].keys():\n",
    "                    c[i][j]=0\n",
    "                c[i][j]+=1\n",
    "    w=dict()\n",
    "    for i,items in c.items():\n",
    "        if i not in w.keys():\n",
    "            w[i]=dict()\n",
    "        for j,cij in items.items():\n",
    "            w[i][j]=c[i][j]/(math.pow(n[i],alpha)*math.pow(n[j],alpha))\n",
    "    return w\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train_data={\n",
    "        'A':{'i1':1,'i2':6 ,'i4':1},\n",
    "        'B':{'i1':1,'i4':1},\n",
    "        'C':{'i1':1,'i2':7,'i5':1},\n",
    "        'D':{'i2':8,'i3':1},\n",
    "        'E':{'i3':1,'i5':1},\n",
    "        'F':{'i2':9,'i4':1}\n",
    "        }\n",
    "    w=itemcos_alpha(train_data)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算用户对商品的推荐分数\n",
    "$$p_{ui}=\\sum\\limits_{N(u)\\bigcap S(j,k)}w_{ij}\\cdot score_{uj}$$\n",
    "$S(j,k)$是与物品$j$相似物品的集合，一般来说j的相似分数最高的$K$个  \n",
    "$score_{uj}$是用户$u$对已购买的物品$j$的评分，如果没有评分数据，则取1\n",
    ">代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T18:12:14.240033Z",
     "start_time": "2019-06-06T18:12:14.182994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i3': 0.37892914162759955, 'i4': 0.7808353265422481}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recommend(train,user_id,w,k):\n",
    "    rank=dict()\n",
    "    user_bought=train[user_id]\n",
    "    for i,scores in user_bought.items():\n",
    "        for j,wj in sorted(w[i].items(),key=lambda x: x[1],reverse=True)[0:k]:\n",
    "            if j in user_bought:##要考虑相似商品里是否已有自购的商品\n",
    "                continue\n",
    "            if j not in rank.keys():\n",
    "                rank[j]=0\n",
    "            rank[j]+=scores*wj\n",
    "    return rank\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    Train_Data = {'A':{'i1':1,'i2':1 ,'i4':1},  \n",
    "     'B':{'i1':1,'i4':1},  \n",
    "     'C':{'i1':1,'i2':1,'i5':1},\n",
    "     'D':{'i2':1,'i3':1},\n",
    "     'E':{'i3':1,'i5':1},\n",
    "     'F':{'i2':1,'i4':1}\n",
    "        }  \n",
    "    w=itemcos_alpha(Train_Data)\n",
    "    rec=recommend(Train_Data,'C',w,3)\n",
    "\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于模型的协同过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关联算法\n",
    ">##### Apriori算法\n",
    ">##### FP Tree算法\n",
    ">##### PrefixSpan算法\n",
    "#### 聚类算法\n",
    "与Item_CF和User_CF类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于矩阵分解的推荐方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 奇异值分解\n",
    "\n",
    "$$A=U\\Sigma V^T$$\n",
    "\n",
    "其中 $U \\in \\mathbb{R}^{m \\times m}$ 的正交矩阵， $\\Sigma \\in \\mathbb{R}^{m \\times n}$ 的对角矩阵，$V \\in \\mathbb{R}^{n \\times n}$ 的正交矩阵\n",
    "\n",
    "$$ \n",
    "A\\begin{bmatrix} \\vdots  & \\vdots  & \\vdots  & \\vdots  \\\\ { v }_{ 1 } & { v }_{ 2 } & \\cdots  & { v }_{ r } \\\\ \\vdots  & \\vdots  & \\vdots  & \\vdots  \\end{bmatrix}=\\begin{bmatrix} \\vdots  & \\vdots  & \\vdots  & \\vdots  \\\\ { u }_{ 1 } & u_{ 2 } & \\cdots  & { u }_{ r } \\\\ \\vdots  & \\vdots  & \\vdots  & \\vdots  \\end{bmatrix}\\begin{bmatrix} { \\sigma  }_{ 1 } & \\quad  & \\quad  & \\quad  & \\quad  \\\\ \\quad  & { \\sigma  }_{ 2 } & \\quad  & \\quad  & \\quad  \\\\ \\quad  & \\quad  & \\quad \\ddots  & \\quad  & \\quad  \\\\ \\quad  & \\quad  & \\quad  & { \\sigma  }_{ r }\\quad  & \\quad  \\\\ \\quad  & \\quad  & \\quad  & \\quad  & \\left( 0 \\right)  \\end{bmatrix} \\\\ {A}{V}={U}{\\Sigma}\n",
    "$$\n",
    "    \n",
    " U和V分别是 $A^TA$ 和$AA^T$的特征向量\n",
    "\\begin{align}\n",
    "& A=U\\Sigma { V }^{ T }\\\\\n",
    "& { A }^{ T }A=V{ \\Sigma  }^{ T }{ U }^{ T }U\\Sigma { V }^{ T }\\\\\n",
    "& \\because \\quad { U }^{ T }U=I\\\\\n",
    "& \\because \\quad { \\Sigma  }^{ T }\\Sigma =\\dots { \\sigma  }_{ i }^{ 2 }\\dots \\\\\n",
    "& { A }^{ T }A=V{ \\Sigma  }^{ T }\\Sigma { V }^{ T }\n",
    "\\end{align}\n",
    "\n",
    "__$A^TA$的特征值是奇异值$\\sigma_i$的平方__  \n",
    "__在矩阵中将奇异值按照降序排序，通常情况下，前10%的奇异值之和就占了全部奇异值之和的99%以上，那么就可以利用前 $r$ 大的奇异值来描述矩阵：__\n",
    "\n",
    "$$ \n",
    "A_{m \\times n} \\approx U_{m\\times r} \\Sigma _{r \\times r} V^T_{r \\times n}\n",
    "$$\n",
    "\n",
    "通过对比奇异值分解后的矩阵和原矩阵，发现值十分接近，说明可以用奇异值分解的矩阵表征原矩阵\n",
    "\n",
    "<span style='color:red'>__将物品的评分矩阵$A^{T}$映射到低维空间$A^{T}U_{m*k}\\Sigma_{k*k}^{I}$中，将维度由$n\\times m$降低到$n\\times k$__</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:10:57.256944Z",
     "start_time": "2019-06-11T09:10:57.183894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3863177  -0.92236578]\n",
      " [-0.92236578  0.3863177 ]] \n",
      " [9.508032   0.77286964] \n",
      " [[-0.42866713 -0.56630692 -0.7039467 ]\n",
      " [ 0.80596391  0.11238241 -0.58119908]\n",
      " [ 0.40824829 -0.81649658  0.40824829]]\n",
      "(10, 11)\n",
      "[14.2701248  11.19808631  7.13480024  5.13612006  4.68588496  3.09682859\n",
      "  2.72917436  2.55571761  1.05782196  0.185364  ]\n",
      "[[ 0.06661544  0.15564622  1.34358251 -0.05523537 -0.30895421  2.49158087\n",
      "   0.81721484 -0.06709356 -0.28593042  0.07729121  4.41737223]\n",
      " [ 0.00698734 -0.20783121 -0.30882512  4.83183689 -0.26688288  2.91916408\n",
      "   0.8764552   0.04106978  0.3290891   0.08690082  2.98423589]\n",
      " [ 0.11810721  0.09600584  0.03439717  0.03531754  4.6109894   0.65869723\n",
      "   1.19552989  0.68277012  0.10922341  2.39556682  0.02920874]\n",
      " [ 2.98329992  1.95337449  2.53776841 -0.54449361 -0.08221392 -0.02968081\n",
      "  -0.04477322  2.7775048   3.00320418 -0.01376337  0.57133206]\n",
      " [ 4.81169574  3.04847219  3.45944129  0.39505758  0.12298993 -0.32749523\n",
      "  -0.14407572  4.56787778  5.0775274   0.06774477 -0.18534872]\n",
      " [-0.0949532  -0.03982373 -0.15421435 -0.15172068  3.87822861  0.35813849\n",
      "   0.941865    0.39195713 -0.10928442  1.9968667  -0.25792377]\n",
      " [ 4.18555696  2.69990683  3.4146455  -0.01892709 -0.06041327  0.22929478\n",
      "   0.02905504  3.9208455   4.28917781  0.0259856   0.92850978]\n",
      " [-0.0524567  -0.17420892  0.14794935  4.09820178 -0.25305933  3.3832142\n",
      "   1.05927282 -0.05557452  0.09398953  0.14437292  4.10009864]\n",
      " [-0.10540254 -0.12834876  0.12408101  2.24341291  1.12264319  2.26364113\n",
      "   0.98996386  0.04527602 -0.0644682   0.75504334  2.64388752]\n",
      " [ 1.12282556  0.48012792 -0.2036592   3.83950834 -0.12899197  0.66100773\n",
      "   0.11651222  1.15295384  1.64455367 -0.03434985 -0.38232734]]\n"
     ]
    }
   ],
   "source": [
    "##svd的使用\n",
    "import numpy as np\n",
    "data=np.mat([[1,2,3],[4,5,6]])\n",
    "u,sigma,vt=np.linalg.svd(data)\n",
    "print(u,'\\n',sigma,'\\n',vt)\n",
    "\n",
    "def loadexdata():\n",
    "    return [[0,0,1,0,0,2,0,0,0,0,5],\n",
    "           [0,0,0,5,0,3,0,0,0,0,3],\n",
    "           [0,0,0,0,4,1,0,1,0,4,0]\n",
    "           [3,3,4,0,0,0,0,2,2,0,0],\n",
    "           [5,4,2,0,0,0,0,5,5,0,0],\n",
    "           [0,0,0,0,5,0,1,0,0,0,0],\n",
    "           [4,1,4,0,0,0,0,4,5,0,1],\n",
    "           [0,0,0,4,0,4,0,0,0,0,4],\n",
    "           [0,0,0,2,0,2,5,0,0,1,2],\n",
    "           [1,0,0,4,0,0,0,1,2,0,0]]\n",
    "\n",
    "mymat=np.mat(loadexdata())\n",
    "print(np.shape(mymat))\n",
    "u,sigma,vt=np.linalg.svd(mymat)\n",
    "print(sigma)\n",
    "newdata=u[:,:4]*np.mat(np.eye(4)*sigma[:4])*vt[:4,:]\n",
    "print(newdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T11:54:49.049417Z",
     "start_time": "2019-06-11T11:54:49.003386Z"
    }
   },
   "source": [
    "#### SVD梳理流程如下：\n",
    ">##### 1、加载用户对物品的评分矩阵\n",
    ">##### 2、矩阵分解，求奇异值，根据奇异值的能量占比确定降至k的数值\n",
    ">##### 3、使用矩阵分解对评分矩阵进行降维度\n",
    ">##### 4、使用降维后的物品评分矩阵计算物品相似度，对用户未评分过的物品进行预测\n",
    ">##### 5、产生前n个评分值高的物品，返回物品编号以及预测评分值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T12:13:26.127310Z",
     "start_time": "2019-06-11T12:13:25.910167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.94993155e-01 -8.55622175e-02 -2.48611806e-02  3.33301232e-02]\n",
      " [-3.13458547e-01 -7.52514984e-02 -7.48135856e-03 -3.50541410e-02]\n",
      " [-3.78450772e-01 -4.30110266e-02 -2.73248525e-02 -3.16127753e-01]\n",
      " [-9.66651163e-02  5.96631453e-01 -1.11181970e-01  7.08108827e-01]\n",
      " [-1.48606103e-02  4.20778007e-02  8.58513789e-01  4.28827005e-02]\n",
      " [-5.34500615e-02  4.98695994e-01  4.12721285e-02 -1.51444295e-01]\n",
      " [-1.53015570e-02  1.73637176e-01  1.96700179e-01 -5.96048540e-02]\n",
      " [-4.69111042e-01 -7.14523528e-02  8.17335025e-02  6.74284466e-02]\n",
      " [-5.18848182e-01 -6.64352384e-02 -3.46135811e-02  1.62309665e-01]\n",
      " [-1.56328052e-02  6.08803417e-02  4.39273573e-01  4.45686018e-04]\n",
      " [-1.06449559e-01  5.79097298e-01 -8.90946777e-02 -5.80592096e-01]]\n",
      "[0 1 2 4 6 7 8 9]\n",
      "item= 0\n",
      "11\n",
      "0.023164930290617003\n",
      "the 0 and 3 similarity is : 0.524508\n",
      "-0.02228585134851963\n",
      "the 0 and 5 similarity is : 0.457935\n",
      "-0.01399325293936381\n",
      "the 0 and 10 similarity is : 0.483310\n",
      "item= 1\n",
      "11\n",
      "-0.038587258416568224\n",
      "the 1 and 3 similarity is : 0.436557\n",
      "-0.015773264017094047\n",
      "the 1 and 5 similarity is : 0.453733\n",
      "0.010808291287677407\n",
      "the 1 and 10 similarity is : 0.520033\n",
      "item= 2\n",
      "11\n",
      "-0.20989356506993934\n",
      "the 2 and 3 similarity is : 0.274214\n",
      "0.0455267804478598\n",
      "the 2 and 5 similarity is : 0.587373\n",
      "0.20135412240977735\n",
      "the 2 and 10 similarity is : 0.744182\n",
      "item= 4\n",
      "11\n",
      "-0.038544193723661246\n",
      "the 4 and 3 similarity is : 0.476120\n",
      "0.05071668220911795\n",
      "the 4 and 5 similarity is : 0.556058\n",
      "-0.07543732018017245\n",
      "the 4 and 10 similarity is : 0.447311\n",
      "item= 6\n",
      "11\n",
      "0.04100029055501607\n",
      "the 6 and 3 similarity is : 0.581131\n",
      "0.10455508321494163\n",
      "the 6 and 5 similarity is : 0.869111\n",
      "0.11926283134794019\n",
      "the 6 and 10 similarity is : 0.766049\n",
      "item= 7\n",
      "11\n",
      "0.04137533883183679\n",
      "the 7 and 3 similarity is : 0.545381\n",
      "-0.017397325934003074\n",
      "the 7 and 5 similarity is : 0.465957\n",
      "-0.037871643834321775\n",
      "the 7 and 10 similarity is : 0.453173\n",
      "item= 8\n",
      "11\n",
      "0.12929847994356275\n",
      "the 8 and 3 similarity is : 0.625646\n",
      "-0.03140796905065142\n",
      "the 8 and 5 similarity is : 0.445549\n",
      "-0.07439312960392347\n",
      "the 8 and 10 similarity is : 0.418503\n",
      "item= 9\n",
      "11\n",
      "-0.010689433459715462\n",
      "the 9 and 3 similarity is : 0.487154\n",
      "0.049258615641346704\n",
      "the 9 and 5 similarity is : 0.605611\n",
      "-0.0024759526020088738\n",
      "the 9 and 10 similarity is : 0.496646\n",
      "[(8, matrix([[3.83996333]])), (7, matrix([[3.74479602]])), (0, matrix([[3.71568381]]))]\n"
     ]
    }
   ],
   "source": [
    "def cossim(u_k,d_t):\n",
    "    num=np.sum(np.multiply(u_k,d_t))##向量各元素相乘后相加\n",
    "    print(num)\n",
    "    denom=np.linalg.norm(u_k)*np.linalg.norm(d_t)\n",
    "    return 0.5+0.5*(num/denom)\n",
    "\n",
    "def svdest(userdata,xformeditems,user,simmeas,item):\n",
    "    n=np.shape(xformeditems)[0]\n",
    "    print(n)\n",
    "    simtotal=0.0\n",
    "    ratsimtotal=0.0\n",
    "    for j in range(n):\n",
    "        userrating=userdata[:,j]\n",
    "        if userrating==0 or j==item:\n",
    "            continue\n",
    "        similarity=simmeas(xformeditems[item,:],xformeditems[j,:])\n",
    "        print('the %d and %d similarity is : %f'%(item,j,similarity))\n",
    "        simtotal+=similarity\n",
    "        ratsimtotal+=similarity*userrating\n",
    "    if simtotal==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return ratsimtotal/simtotal\n",
    "\n",
    "def recommend(datamat,user,N=3,simmeas=cossim,estmethod=svdest):\n",
    "    u,sigma,vt=np.linalg.svd(datamat)\n",
    "    sig4=np.mat(np.eye(4)*sigma[:4])\n",
    "    xformeditems=datamat.T*u[:,:4]*sig4.I\n",
    "    print(xformeditems)\n",
    "    unrateditems=np.nonzero(datamat[user,:].A==0)[1]##.A是指将矩阵定义为array\n",
    "    print(unrateditems)\n",
    "    if len(unrateditems)==0:\n",
    "        return 'rated everything'\n",
    "    itemscores=[]\n",
    "    for item in unrateditems:\n",
    "        print('item=',item)\n",
    "        estimatedscore=estmethod(datamat[user,:],xformeditems,user,simmeas,item)\n",
    "        itemscores.append((item,estimatedscore))\n",
    "    return sorted(itemscores,key=lambda x:x[1],reverse=True)[:N]\n",
    "\n",
    "def loadexdata():\n",
    "    return [[0,0,1,0,0,2,0,0,0,0,5],\n",
    "           [0,0,0,5,0,3,0,0,0,0,3],\n",
    "           [0,0,0,0,4,1,0,1,0,4,0],\n",
    "           [3,3,4,0,0,0,0,2,2,0,0],\n",
    "           [5,4,2,0,0,0,0,5,5,0,0],\n",
    "           [0,0,0,0,5,0,1,0,0,0,0],\n",
    "           [4,1,4,0,0,0,0,4,5,0,1],\n",
    "           [0,0,0,4,0,4,0,0,0,0,4],\n",
    "           [0,0,0,2,0,2,5,0,0,1,2],\n",
    "           [1,0,0,4,0,0,0,1,2,0,0]]\n",
    "mymat=np.mat(loadexdata())\n",
    "result=recommend(mymat,1,estmethod=svdest)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 隐语义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVD的缺点：__\n",
    ">1、SVD计算前会将评分矩阵的缺失值补全，补全后需要巨大的存储空间，在实际上，用户对物品的行为信息不止千万，如此进行存储是不现实的；  \n",
    ">2、SVD计算复杂度高，因此很多SVD的研究只能在小数据集上进行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐语义模型是将原始矩阵分解为两个矩阵：\n",
    "$$A=PQ^{T}$$\n",
    "$P$为用户因子矩阵，$Q$为物品因子矩阵  \n",
    "分解时通过优化损失函数来找到合适参数，其中$r_{ij}$为用户$i$对物品$j$的评分:\n",
    "$$min( \\left \\| r_{ij}-\\sum\\limits^{K}_{i=1}p_{ik}q_{ik} \\right \\|^2_{2}+\\lambda \\left \\| p_{i} \\right \\| ^2+\\gamma \\left \\| q_{j} \\right \\|^2)$$\n",
    "该方法也成为ALS，<span style='color:red'>而ALS在对隐式反馈模型进行处理时，还需要考虑“加权正则化矩阵分解”：</span>\n",
    "$$min( \\left \\| {\\color{Red}{c_{ij}}} r_{ij}-\\sum\\limits^{K}_{i=1}p_{ik}q_{ik} \\right \\|^2_{2}+\\lambda \\left \\| p_{i} \\right \\| ^2+\\gamma \\left \\| q_{j} \\right \\|^2)$$\n",
    "在隐式反馈模型中是没有评分的，上式的$r_{ij}$并不是具体的分数，而<span style='color:red'>仅为1</span>，表示用户和物品之间有交互，不能表示评分高低或喜好程度。而$c_{ij}$可以表示用户偏爱某个商品的置信程度，如交互次数多的权重就会增加。如果用$d_{ij}$来表示交互次数的话，置信程度可以表示为：\n",
    "$$c_{ij}=1+\\alpha d_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ALS的过程：\n",
    ">1、初始化Q，求P的最佳值  \n",
    ">2、固定P，求Q的最佳值  \n",
    ">3、固定Q，求P的最佳值  \n",
    ">4、循环2和3步  \n",
    "##### 对于大数据及使用Spark的MLlib库中的ALS api进行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "\n",
    "case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)\n",
    "\n",
    "##读取数据集，使用,分隔，分别为用户id，物品id，评分，次数\n",
    "def parseRating(str: String): Rating = {\n",
    "  val fields = str.split(\",\")\n",
    "  assert(fields.size == 4)\n",
    "  Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)\n",
    "}\n",
    "\n",
    "val ratings = spark.read.textFile(\".../als/movielens_ratings.txt\")\n",
    "  .map(parseRating)\n",
    "  .toDF()\n",
    "\n",
    "##拆分训练集和测试集\n",
    "val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "##rating：由用户-物品矩阵构成的训练集\n",
    "##rank：隐藏因子的个数\n",
    "##numIterations: 迭代次数\n",
    "##lambda：正则项的惩罚系数\n",
    "##alpha： 置信参数\n",
    "val als = new ALS()\n",
    "  .setRank(100)\n",
    "  .setMaxIter(50)\n",
    "  .setRegParam(0.01)\n",
    "  .setUserCol(\"userId\")\n",
    "  .setItemCol(\"movieId\")\n",
    "  .setRatingCol(\"rating\")\n",
    "val model = als.train(training)\n",
    "\n",
    "\n",
    "##rating：由用户-物品矩阵构成的训练集\n",
    "##rank：隐藏因子的个数\n",
    "##numIterations: 迭代次数\n",
    "##lambda：正则项的惩罚系数\n",
    "##alpha： 置信参数\n",
    "val als = new ALS()\n",
    "  .setRank(100)\n",
    "  .setMaxIter(50)\n",
    "  .setRegParam(0.01)\n",
    "  .setUserCol(\"userId\")\n",
    "  .setItemCol(\"movieId\")\n",
    "  .setRatingCol(\"rating\")\n",
    "val model2 = als.trainImplicit(training)\n",
    "\n",
    "##在测试集上进行预测\n",
    "val predictions = model.predict(test)\n",
    "\n",
    "##获得物品的特征\n",
    "val item_feature = model.productFeatures\n",
    "\n",
    "##获得用户的特征\n",
    "val user_feature = model.userFeatures\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 隐语义模型的优点：相比SVD，ALS能有效地解决过拟合问题，且基于协同过滤算法的可扩展性也优于SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于稀疏自编码的推荐方法\n",
    "#### 基础自编码结构\n",
    "- 构造一个三层的神经网络，中间隐藏层的维度远远低于输入层和输出层\n",
    ">当隐藏神经元的数量较大（为了更有效找出隐含在输入数据里的内部结构与模式，会寻找一组超完备基向量），也可通过给神经网络施加限制，如激活函数等来做稀疏性限制\n",
    "- 自编码神经网络尝试去学习一个$h_{W,b}(x)\\approx x$的函数\n",
    "#### 多层结构\n",
    "- 一个输入层、两个隐藏层、一个输出层的栈式自编码网络  \n",
    "<img src='稀疏自编码网络.png'>  \n",
    ">+1是为了保留截距项，可以重构输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于社交网络的推荐算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 社交网络结构\n",
    ">社交图谱  \n",
    ">兴趣图谱  \n",
    "- 社交网络数据类型\n",
    ">双向确认的社交网络数据（如微信用户数据）（可通过无向图表示）  \n",
    ">单向关注的社交网络数据（如微博用户数据）（可通过有向图表示）  \n",
    ">基于社区的社交网络数据（如知乎中的“话题”中的用户）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于用户的推荐在社交网络中的应用\n",
    "- 在社交网络中，可以用图$G(V,E,W)$来定义一个社交网络：其中$V$是顶点集合，每个顶点代表一个用户；E是边集合，如果用户$V_{a}$和用户$V_{b}$有社交网络关系，则有一条边$e(V_{a},V_{b})$连接这两个用户；$W(V_{a},V_{b})$定义为边的权重  \n",
    "- $out(u)$定义为<span style='color:red'>顶点$u$指向</span>的顶点集合，$in(u)$<span style='color:red'>指向顶点$u$</span>的顶点集合  \n",
    "#### 相似度计算方法\n",
    "##### 对于用户$u$和用户$v$，可以使用共同好友比例计算他们的相似度\n",
    "$$w_{1}(u,v)= \\frac{\\left| out(u) \\bigcap out(v) \\right| }{\\sqrt{\\left| out(u) \\right| \\cdot \\left| out(v)\\right|}}$$\n",
    "##### 使用共同被关注的用户数量来计算用户之间的相似度\n",
    "$$w_{1}(u,v)= \\frac{\\left| in(u) \\bigcap in(v) \\right| }{\\sqrt{\\left| in(u) \\right| \\cdot \\left| in(v)\\right|}}$$\n",
    "***$w_1$更适合于普通用户的兴趣相似度，因为普通用户往往关注行为更丰富；$w_2$往往更适于大$V$之间的相似度，因为大$V$往往被关注的人数比较多***\n",
    "##### 使用自身关注好友的关注用户量来计算相似度\n",
    "$$w_{1}(u,v)= \\frac{\\left| out(u) \\bigcap in(v) \\right| }{\\sqrt{\\left| out(u) \\right| \\cdot \\left| in(v)\\right|}}$$\n",
    "***<span style='color:red'>当没有没有分母上的$in(v)$时，所有人与大$V$的相似度就会非常大</span>***\n",
    "#### 新的相似度分数\n",
    "$$w^{'}(u,v)=\\theta w_{baseNet}(u,v)+(1-\\theta)w_{baseUser}(u,v)$$\n",
    "\n",
    "$$p_{ui}=\\sum\\limits_{N(i)\\bigcap S(u,k)}w^{'}(u,v)\\cdot score_{vi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Spark中的Graphx模块计算可以比python提升3倍以上计算效率\n",
    "代码\n",
    "```Spark\n",
    "import org.apache.spark.graphx.{GraphLoader, VertexRDD}                     \n",
    "import org.apache.spark.{SparkConf, SparkContext}                           \n",
    "                                                                            \n",
    "object GraphRale {                                                          \n",
    "  /**                                                                       \n",
    "    * 数据列表的笛卡尔乘积：{1,2,3,4}=>{(1,2),(1,3),(1,4),(2,3),(2,4),(3,4）}           \n",
    "    * @param input                                                          \n",
    "    * @return                                                               \n",
    "    */                                                                      \n",
    "  def ciculate(input:List[Long]):Set[String]={                              \n",
    "    var result = Set[String]()                                              \n",
    "    input.foreach(x=>{                                                      \n",
    "      input.foreach(y=>{                                                    \n",
    "        if(x<y){                                                            \n",
    "          result += s\"${x}|${y}\"                                            \n",
    "        }else if(x>y){                                                      \n",
    "          result += s\"${y}|${x}\"                                            \n",
    "        }                                                                   \n",
    "      })                                                                    \n",
    "    })                                                                      \n",
    "    return result;                                                          \n",
    "  }                                                                         \n",
    "  def twoDegree()={                                                         \n",
    "    val conf = new SparkConf().setMaster(\"common friends\").setAppName(\"graph\")       \n",
    "    val sc = new SparkContext(conf)                                \n",
    "    ##输入数据为好友关系对，每一行为2个id\n",
    "    val graph = GraphLoader.edgeListFile(sc,\"./newtwork/grap.txt\")   \n",
    "    ##将形如(a,b),(a,c),(a,f)的关系对转化为a->list(b,c,f) \n",
    "    val relate: VertexRDD[List[Long]] = graph.aggregateMessages[List[Long]](\n",
    "      triplet=>{                                                            \n",
    "        triplet.sendToDst(List(triplet.srcId))                              \n",
    "      },                                                                    \n",
    "      (a,b)=>(a++b)                                                         \n",
    "    ).filter(x=>x._2.length>1)                                              \n",
    "    ##a->list(b,c,f)中bcd用户均有共同好友a                                                                        \n",
    "    val re = relate.flatMap(x=>{                                            \n",
    "      for{temp <- ciculate(x._2)}yield (temp,1)                             \n",
    "    }).reduceByKey(_+_)                                                     \n",
    "                                                                            \n",
    "    re.foreach(println(_))                                                  \n",
    "  }                                                                         \n",
    "  def main(args: Array[String]): Unit = {                                   \n",
    "    twoDegree()                                                             \n",
    "  }                                                                         \n",
    "}       \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node2vec技术在社交网络推荐中的应用\n",
    "- 使用基于用户社交网络计算用户相似度的方法是使用离线计算好用户的相似度并存储下来供线上推荐系统使用，显然这是不合理的，因此使用坐标来描述用户在社交网络中的位置是更合理的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random walk\n",
    "定义$p$和$q$两个参数变量来调节游走\n",
    "$$\\begin{cases}\n",
    "1/p,d=0 \\\\\n",
    "1,d=1 \\\\ \n",
    "1/q,d=2\n",
    "\\end{cases}$$\n",
    "一般来说，我们会从每个点开始游走5~10次，步长则根据点的数量$N$游走$\\sqrt{N}$步\n",
    "#### Work2vec\n",
    "##### Skip-Gram\n",
    "- 训练神经网络后所得到的权重即为词向量\n",
    ">1、建立模型：基于训练数据建立神经网络，取input word和2$\\times$skipwindow 的的词，类似于（world）->（the，whole，is，about）  \n",
    ">2、训练模型获取词向量\n",
    "##### CBOW\n",
    "- 与Skip-Gram相反，利用上下文来推测中心词\n",
    "##### 评估词向量的质量：word analogy\n",
    "将训练出的Word embedding通过加减法操作，来对应某种关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推荐系统冷启动问题\n",
    "### 用户冷启动\n",
    "#### 使用账户信息\n",
    "在构建推荐系统时，接入公开的SDK（如QQ、微信、微博等）有效利用用户的账户信息，查看用户使用快速登录的其他平台的行为，作为参考\n",
    ">通过  注册开发者->创建应用->完善信息并获得接口\n",
    "#### 使用手机IMEI号\n",
    "iPhone也有类似于安卓的接口获取OpenUDID来区分不同的设备\n",
    "#### 制造选项\n",
    "让用户自己选择感兴趣的point，及时生成粗粒度的推荐\n",
    "***需要注意的是，这种方法不自然，用户体验较差，但如果给予好的设计，将可以吸引用户去选择感兴趣的点（如QQ音乐的用户偏好选择页面）***\n",
    "### 物品冷启动\n",
    "#### 利用物品的内容信息\n",
    "使用TF-IDF算法来计算物品的相似度\n",
    "#### 利用专家的标注数据\n",
    "定义专家：他们必须是在一个特定领域内，能对该领域的条目给出深思熟虑的、一致的、可靠的评价（打分）\n",
    "- 缺点：对于初创公司或刚刚启动的APP，没有能力也没有足够的资金来获得大量的专家数据\n",
    "### 深度学习技术在物品冷启动上的应用\n",
    "#### 案例一：CNN在音频流派分类上的应用\n",
    "一首歌对应一个音频文件，经典的采样频率为44100Hz--每秒音频存储44100个值，而立体声则为两倍，相当于3分钟的歌曲有7938000个样本，因此可暂时将立体声声道丢弃。\n",
    "借助libsora工具可以将音频数据转为PNG文件，其中包含歌曲所有的频率随着时间的变化。\n",
    "使用50像素（每像素20ms）以降低PNG图片的分辨率并将图片分割成10~15s的片段（一般来说10s足以判断音乐的分类）。  \n",
    "- 缺点：CNN模型不完全适用于音频分类\n",
    ">一般图片有invariance（即不变形，图片在旋转后不会对分类有影响，但音频的频谱图不是这样的）  \n",
    ">CNN通过fiter size获取前后信息，受限于size的大小，long dependence不如LSTM\n",
    "- 改善方法：CNN+LSTM\n",
    "将样本经过两侧卷积层之后，把不同通道上相同时序的特征组合起来作为LSTM的输入，最后通过全连接进一步提取音频分类特征  \n",
    "***除了使用流派分类以外，还可以将模型倒数第二层的全连接层的向量作为歌曲特征***  \n",
    "代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 案例二：人脸魅力值打分在视频推荐中的应用\n",
    "使用ResNet做迁移学习fine-tuning可以达到较好的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTR预估算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM（因子分解机）算法\n",
    "- 经过One-Hot编码之后，大部分样本数据特征是比较稀疏的，因此FM算法的提出在解决大规模稀疏数据下的特征组合问题\n",
    "- 某些特征经过关联之后，与 label 之间的相关性就会提高，因而提出二项式模型：\n",
    "$$y(x)=w_0+\\sum_{i=1}^nw_ix_i+\\sum_{i=1}^n\\sum_{j=i+1}^nw_{ij}x_ix_j$$\n",
    "其中，$n$代表样本的特征数量，$x_{i}$ 是第$i$个特征的值,$w_0、w_i、w_{ij}$是模型的参数  \n",
    "***在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练时困难的，因为<span style='color:red'>交叉项的每一个参数$w_{ij}$的学习过程需要大量的$xi、xj$同时非零的训练样本数据</span>，由于样本数据本来就很稀疏，所以训练样本不充分，从而影响模型预测的效果和稳定性。***  \n",
    "参照矩阵分解，所有二次项参数$w_{ij}$可以组成一个对称矩阵$W$，则这个矩阵可以分解为$W=V^TV$，$V$的$j$列便是第$j$维特征的隐向量，则每个参数$w_{ij}=⟨v_i,v_j⟩$。因此，FM模型方程为\n",
    "$$y(x)=w_0+\\sum_{i=1}^nw_{i}x_{i}+\\sum_{i=1}^n\\sum_{j=i+1}^n ⟨vi,vj⟩x_{i}x_{j}$$\n",
    "其中，$v_{i}$是第$i$维特征的隐向量，$⟨⋅,⋅⟩$代表向量点积，计算公式为\n",
    "$$⟨v_i,v_j⟩=\\sum_{f=1}^kv_{i,f}·v_{j,f}$$\n",
    ">模型求解时不直接求特征交叉项的系数$w_{ij}$（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积$⟨v_{i},v_{j}⟩$表示$w_{ij}$\n",
    ">>具体的，对每一个特征分量$x_{i}$引入隐向量$v_i＝(v_{i,1},v_{i,2},⋯,v_{i,k})$，利用$v_{i}v^T_{j}$内积结果对交叉项的系数$w_{ij}$ 进行估计，公式表示：$ŵ_{ij}=v_iv^T_j$\n",
    ">>>直观上看，FM的复杂度为$O(kn^2)$，但通过以下转换，可以进行化简复杂度为$O(kn)$：\n",
    "$$\\sum_{i=1}^n\\sum_{j=i+1}^n⟨v_i,v_j⟩x_ix_j \\\\ =\\frac{1}{2}\\sum_{i=1}^n\\sum_{f=1}^n⟨v_i,v_j⟩x_ix_j-\\frac{1}{2}\\sum_{i=1}^n⟨v_i,v_i⟩x_ix_i\\\\=\\frac{1}{2}(\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{f=1}^kv_{i,f}v_{j,f}x_ix_j-\\sum_{i=1}^n\\sum_{f=1}^kv_{i,f}v_{i,f}x_ix_i)\\\\=\\frac{1}{2}\\sum_{f=1}^k[(\\sum_{i=1}^nv_{i,f}x_i)·(\\sum_{j=1}^nv_{j,f}x_j)-\\sum_{i=1}^nv_{i,f}^2x_i^2]\\\\=\\frac{1}{2}\\sum_{f=1}^k[(\\sum_{i=1}^nv_{i,f}x_i)^2- \\sum_{i=1}^nv_{i,f}^2x_i^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFM（场感知分解机）算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T11:54:14.213332Z",
     "start_time": "2019-06-20T11:54:14.162297Z"
    }
   },
   "source": [
    "### DeepFM算法\n",
    "详见“基于深度学习的推荐模型”之 \n",
    "[“基于DeepFM的推荐算法”](#基于DeepFM的推荐算法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于深度学习的推荐模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于DeepFM的推荐算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推荐系统架构设计\n",
    "- 推荐系统作用\n",
    ">提升用户体验（帮助用户快速找到感兴趣的内容）  \n",
    ">提高产品销售（帮助用户和产品简历精准连接，提高产品转化率）  \n",
    ">发掘长尾价值  \n",
    ">方便移动互联网用户交互（通过推荐减少用户操作，主动找到用户感兴趣的内容）\n",
    "### 推荐系统的基本模型\n",
    ">协同模型  \n",
    ">内容模型  \n",
    ">知识模型\n",
    ">>通过用户的限定条件，按照他的需求进行推荐\n",
    "### 推荐系统常见架构\n",
    "#### 基于离线训练的推荐系统架构设计\n",
    "##### 常见算法\n",
    ">逻辑回归、梯度提升决策树、因子分解机\n",
    "##### 离线系统架构组成\n",
    ">数据上报和离线训练组成学习系统，实时计算和A/B测试组成预测系统  \n",
    ">训练的数据流搜集业务数据，最后生成模型存储于在线存储模块；预测的数据流接受业务的预测请求，通过A/B测试模块访问实时计算模块获取预测结果\n",
    "###### 数据上报\n",
    ">收集  \n",
    ">>业务驱动，从物品、用户、场景几个维度收集，保证数据质量，量化一切，越细越好   \n",
    "\n",
    " >验证  \n",
    ">>对数据进行准确性验证，避免逻辑错误、数据错位、数据缺失等问题  \n",
    "\n",
    " >清洗  \n",
    ">>空值检查、数值异常、类型异常、数据去重等\n",
    "\n",
    " >转换  \n",
    ">>将数据转换为训练需要的样本格式，保存到离线存储模块\n",
    "###### 离线训练\n",
    ">###### 离线存储\n",
    ">使用分布式文件系统或存储平台来存储数据\n",
    ">###### 离线计算\n",
    ">样本抽样\n",
    ">>对于样本不均衡情况，一方面可以通过惩罚权重和组合等方法解决，一方面要结合业务理解  \n",
    ">>***对于恶意刷流量、机器人用户，通过样本去重保证用户样本数的均衡***\n",
    "\n",
    " >特征工程\n",
    " >>特征选择\n",
    " >>>通过评价函数、停止准则、验证过程等步骤，从特征集合中挑选一组最具统计意义的特征子集\n",
    " \n",
    " >>特征提取\n",
    " >>>通过成分分析、判别分析等方法对原始特征进行变换和组合，构建具有业务或同级意义的核心特征\n",
    " \n",
    " >>特征组合\n",
    " >>>通过多模态embedding等方法，将来自用户、物品和背景的特征向量组合在一起，达到信息互补\n",
    " \n",
    " >模型训练\n",
    " >>实际业务中，考虑到需要处理大规模的训练集，一般会选择分布式训练的近似线性时间的算法\n",
    " \n",
    " >相似度计算\n",
    "###### 在线存储\n",
    ">为了使在线存储尽可能地快，在开源软件的基础上，可以进行一些定制：缓存策略、延迟过期策略、使用固态硬盘等\n",
    "###### 实时推荐\n",
    "- 因为需要快速响应，该模块需要一个分布式的计算框架来完成计算任务\n",
    "- ###### 实时计算步骤\n",
    ">###### 从在线模块中读取用户画像及历史行为，构建出该用户的模型特征\n",
    ">###### 调用推荐模型，结合用户特征调用推荐系统的算法模型，得出喜好概率\n",
    ">###### 对候选池的打分结果进行排序并返回结果\n",
    "\n",
    "- ###### 常见实时推荐做法\n",
    ">###### 一般分为召回和排序两步（与上面实时计算相似）。<span style='color:red'>为了多样性和运营的一些考虑,使用重排过滤给用户一些探索性的内容，避免用户在平台上看到的内容过于同质化</span>\n",
    "###### A/B测试\n",
    ">__A/B测试的目的是通过流量分割和科学采样，<span style='color:green'>在小流量测试</span>中获得具有代表性的实验理论__\n",
    "###### 需要注意的问题\n",
    ">###### 推荐结果反馈\n",
    ">###### 模型更新的健壮性\n",
    ">程序开发的时候，需要在模型计算得时候，考虑即使在某些特征缺失或者不匹配的情况下，也能够最大程度上返回较准确的计算结果\n",
    ">###### 海量服务\n",
    "推荐系统服务于海量的用户，所以线上服务需要做到高可靠、高吞吐、低延迟，因此可以使用优化方案：\n",
    ">>过载保护：对突发性流量进行过载保护  \n",
    ">>流式计算: 通过分布式流失计算框架应对大量在线请求  \n",
    ">>共享内存组件，使得在线存储部分的性能开销尽可能降低\n",
    "\n",
    " >###### 通用性设计\n",
    " >- 为了使不同算法可以用于不同样本和特征，也可以使用一个算法配置来存储数据、算法和模型的映射关系\n",
    " >设计出的一套推荐系统要能支持众多的业务和场景，设计部分：\n",
    " >>样本库：存储用户行为和特征\n",
    " >>特征库：存储用户和物品的属性等特征\n",
    " >>算法：存储机器学习算法\n",
    " >>模型：存储从样本和特征计算得到的训练模型\n",
    " \n",
    " >###### 用户画像\n",
    " 用户画像对于业务了解用户具有非常重大的意义，可以帮助 大幅度的提升推荐准确度"
   ]
  }
 ],
 "metadata": {
  "author": "m",
  "kernelspec": {
   "display_name": "tensorflow35",
   "language": "python",
   "name": "tensorflow35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
