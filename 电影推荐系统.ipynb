{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用TensorFlow实现矩阵分解\n",
    "## 数据导入与数据观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:03.107074Z",
     "start_time": "2019-07-13T15:32:28.218140Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "col_names = ['user', 'item', 'rate', 'timestamp']\n",
    "df = pd.read_csv('./movielens/ml-1m/ratings.dat', sep = '::', header = None, names = col_names, engine = 'python')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:04.171791Z",
     "start_time": "2019-07-13T15:33:03.772515Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = ['user']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:05.757843Z",
     "start_time": "2019-07-13T15:33:04.643097Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = ['item']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:06.495336Z",
     "start_time": "2019-07-13T15:33:06.385262Z"
    }
   },
   "outputs": [],
   "source": [
    "df['user'] -= 1\n",
    "df['item'] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:07.255844Z",
     "start_time": "2019-07-13T15:33:07.164781Z"
    }
   },
   "outputs": [],
   "source": [
    "df['rate'] = df['rate'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:08.189031Z",
     "start_time": "2019-07-13T15:33:08.142150Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df['user'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:08.847282Z",
     "start_time": "2019-07-13T15:33:08.784456Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df['item'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:10.228120Z",
     "start_time": "2019-07-13T15:33:09.470817Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_data():\n",
    "    df = read_and_process(\"./movielens/ml-1m/ratings.dat\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop = True)##打乱数据\n",
    "    split_index = int(rows*0.9)\n",
    "    df_train = df[0: split_index]\n",
    "    df_test = df[split_index:]\n",
    "    return df_train, df_test\n",
    "\n",
    "def read_and_process(filename, sep = '::'):\n",
    "    col_names = ['user', 'item', 'rate', 'timestamp']\n",
    "    df = pd.read_csv(filename, sep = sep, header = None, names = col_names, engine = 'python')\n",
    "    df['user'] -= 1\n",
    "    df['item'] -= 1\n",
    "    for col in ('user', 'item'):\n",
    "        df[col] = df[col].astype('int32')\n",
    "    df['rate'] = df['rate'].astype('float32')\n",
    "    return df\n",
    "    \n",
    "class ShuffleDataIterator:\n",
    "    def __init__(self, inputs, batch_size = 10):\n",
    "        ##注意这里的输入\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        print([np.array(self.inputs[i]) for i in range(self.num_cols)])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size))\n",
    "        out = self.inputs[ids, :]\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "    \n",
    "class OneEpochDataIterator(ShuffleDataIterator):\n",
    "    def __init__(self, inputs, batch_size = 10):\n",
    "        super().__init__(inputs, batch_size = batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "        \n",
    "        def next(self):\n",
    "            if self.group_id >= len(self.idx_group):\n",
    "                self.group_id = 0\n",
    "                raise StopIteration\n",
    "            out = self.inputs[self.idx_group[self.group_id], :]\n",
    "            self.group_id += 1\n",
    "            return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络和迭代优化部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:23.068066Z",
     "start_time": "2019-07-13T15:33:10.898571Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:23.431503Z",
     "start_time": "2019-07-13T15:33:23.073067Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference_svd(user_batch, item_batch, user_num, item_num, dim = 5, device = '/cpu:0'):\n",
    "    with tf.device('/cpu:0'):\n",
    "        global_bias = tf.get_variable('global_bias', shape = [])\n",
    "        w_bias_user = tf.get_variable('embd_bias_user', shape = [user_num])\n",
    "        w_bias_item = tf.get_variable('embd_bias_item', shape = [item_num])\n",
    "        \n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name = 'bias_user')\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name = 'bias_item')\n",
    "        \n",
    "        w_user = tf.get_variable('embd_user', shape = [user_num, dim], initializer = tf.truncated_normal_initializer(stddev = 0.02))\n",
    "        w_item = tf.get_variable('embd_item', shape = [item_num, dim], initializer = tf.truncated_normal_initializer(stddev = 0.02))\n",
    "        \n",
    "        embd_user = tf.nn.embedding_lookup(w_user, user_batch, name = 'embedding_user')\n",
    "        embd_item = tf.nn.embedding_lookup(w_item, item_batch, name = 'embedding_item')\n",
    "    \n",
    "    with tf.device(device):\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)##tf.multiply是元素点乘\n",
    "        infer = tf.add(infer, global_bias)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name = 'svd_inference')\n",
    "        \n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), name = 'svd_regularization')\n",
    "    return infer, regularizer\n",
    "\n",
    "def optimizer(infer, regularizer, rate_batch, learning_rate = 0.001, reg = 0.1, device = '/cpu:0'):\n",
    "    global_step = tf.train.get_global_step()\n",
    "    assert global_step is not None\n",
    "    with tf.device(device):\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype = tf.float32, shape = [], name = 'l2')\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step = global_step)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:26.106835Z",
     "start_time": "2019-07-13T15:33:24.842439Z"
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from six import next\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.framework import summary_pb2\n",
    "\n",
    "np.random.seed(12321)\n",
    "\n",
    "batch_size = 2000\n",
    "user_num = len(df['user'].unique())\n",
    "item_num = len(df['item'].unique())\n",
    "dim = 15\n",
    "epoch_max = 200\n",
    "device = '/cpu:0'\n",
    "\n",
    "def make_scalar_summary(name, val):\n",
    "    return summary_pb2.Summary(value = [summary_pb2.Summary.Value(tag = name, simple_value = val)])\n",
    "\n",
    "def svd(train, test):\n",
    "    samples_per_batch = len(train) // batch_size\n",
    "    \n",
    "    iter_train = ShuffleDataIterator([train['user'], train['item'], train['rate']], batch_size = batch_size)##注意iuputs\n",
    "    iter_test = OneEpochDataIterator([test['user'], test['item'], test['rate']], batch_size = -1)\n",
    "    \n",
    "    user_batch = tf.placeholder(tf.int32, shape = [None], name = 'id_user')\n",
    "    item_batch = tf.placeholder(tf.int32, shape = [None], name = 'id_item')\n",
    "    rate_batch = tf.placeholder(tf.float32, shape = [None])\n",
    "    \n",
    "    infer, regularizer = inference_svd(user_batch, item_batch, user_num = user_num, item_num = item_num, dim = dim)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    cost, train_op = optimizer(infer, regularizer, rate_batch, learning_rate = 0.001, reg = 0.05, device = device)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir = './data', graph = sess.graph)\n",
    "        print('{}{}{}{}'.format('epoch', 'train_error', 'val_error', 'elapsed_time'))\n",
    "        errors = deque(maxlen = samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(epoch_max * samples_per_batch):\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, perd_batch = sess.run([train_op, infer], feed_dict = {user_batch: users, item_batch: items, rate_batch: rates})\n",
    "            pred_batch = np.clip(pred_batch, 1.0, 5.0)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0:\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                for users, items, rates in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict = {user_batch: users, item_batch: items})\n",
    "                    pred_batch = np.clip(pred_batch, 1.0, 5.0)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates), 2)\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                print('{:3d}{:f}{:f}{:f}(s)'.format(i // samples_per_batch, train_err, test_err, end - start))\n",
    "                train_err_summary = make_scalar_summary('training_error', train_err)\n",
    "                test_err_summary = make_scalar_summary('testing_error', test_err)\n",
    "                summary_writer.add_summary(train_err_summary, i)\n",
    "                summary_writer.add_summary(test_err_summary, i)\n",
    "                start = end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T15:33:40.260306Z",
     "start_time": "2019-07-13T15:33:26.112839Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test = get_data()\n",
    "svd(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用surprise库实现电影推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T11:06:19.640113Z",
     "start_time": "2019-07-12T10:50:30.652128Z"
    }
   },
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader(line_format = 'user item rating timestamp', sep = '::')\n",
    "data = Dataset.load_from_file('./movielens/ml-1m/ratings.dat', reader = reader)\n",
    "algo = KNNBaseline()\n",
    "perf = cross_validate(algo, data, measures = ['RMSE', 'MAE'], cv = 3, verbose = True)\n",
    "##verbose = True是指打印出过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T16:15:24.133935Z",
     "start_time": "2019-07-12T16:15:24.109921Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./movielens/ml-1m/movies.dat', 'r', encoding = 'ISO-8859-1') as f:\n",
    "    movies_id_dic = {}\n",
    "    id_movies_dic = {}\n",
    "    for line in f.readlines():\n",
    "        movies = line.strip().split('::')\n",
    "        id_movies_dic[int(movies[0]) - 1] = movies[1]\n",
    "        movies_id_dic[movies[1]] = int(movies[0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T16:15:25.259160Z",
     "start_time": "2019-07-12T16:15:25.250156Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_id = int(movies_id_dic['Toy Story (1995)'])\n",
    "print(movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T16:15:28.561215Z",
     "start_time": "2019-07-12T16:15:28.264594Z"
    }
   },
   "outputs": [],
   "source": [
    "toy_story_neighbors = algo.get_neighbors(movie_id, k = 10)\n",
    "print(toy_story_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T16:15:58.923090Z",
     "start_time": "2019-07-12T16:15:58.911085Z"
    }
   },
   "outputs": [],
   "source": [
    "print('最接近《Toy Story (1995)》的10部电影是：')\n",
    "for i in toy_story_neighbors:\n",
    "    print(id_movies_dic[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用pyspark实现矩阵分解与预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置spark的运行环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T03:59:54.121007Z",
     "start_time": "2019-07-12T03:58:46.947231Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('movielenALS').set('spark.excutor.memory', '2g')\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据和将数据转换为RDD格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T04:38:26.790664Z",
     "start_time": "2019-07-12T04:38:25.543170Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_data = sc.textFile('./movielens/ml-1m/ratings.dat')\n",
    "print(ratings_data.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T04:38:28.687093Z",
     "start_time": "2019-07-12T04:38:27.885436Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_int = ratings_data.map(lambda x: x.split('::')[0:3])\n",
    "print(ratings_int.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T04:38:48.637195Z",
     "start_time": "2019-07-12T04:38:45.208473Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "\n",
    "rates_data = ratings_int.map(lambda x: Rating(int(x[0]), int(x[1]), int(x[2])))\n",
    "print(rates_data.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "### 预测user14对item25的评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T04:49:11.647928Z",
     "start_time": "2019-07-12T04:48:45.682940Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('checkpoint/')\n",
    "ALS.checkpointInterval = 2\n",
    "model = ALS.train(ratings = rates_data, rank = 20, iterations = 5, lambda_ = 0.02)\n",
    "print(model.predict(14, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测item25的最值得推荐的10个user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T04:50:44.726165Z",
     "start_time": "2019-07-12T04:50:44.003930Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.recommendUsers(25, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测user14的最值得推荐的10个item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T04:51:59.700682Z",
     "start_time": "2019-07-12T04:51:59.608620Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.recommendProducts(14, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测出每个user最值得被推荐的3个item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T04:53:50.378312Z",
     "start_time": "2019-07-12T04:53:46.925239Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.recommendProductsForUsers(3).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测出每个item最值得被推荐的3个user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T04:54:57.878119Z",
     "start_time": "2019-07-12T04:54:55.436791Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.recommendUsersForProducts(3).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow35",
   "language": "python",
   "name": "tensorflow35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
